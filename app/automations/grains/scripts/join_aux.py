import os
import pandas as pd
import logging
import asyncio

# Set up logging
# Set BASE_DIR to point to the root of the 'app' folder
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# Define the correct paths for the grains module
GRAINS_DIR = os.path.join(BASE_DIR, 'automations', 'grains')
DATA_DIR = os.path.join(GRAINS_DIR, 'data')
LOGS_DIR = os.path.join(GRAINS_DIR, 'logs')

# Set up logging
log_file = os.path.join(LOGS_DIR, 'join_aux.log')
os.makedirs(LOGS_DIR, exist_ok=True)
logging.basicConfig(level=logging.DEBUG, filename=log_file, 
                    format='%(asctime)s - %(levelname)s - %(message)s')

def convert_columns_to_str(df, columns):
    """
    Convert the specified columns in a DataFrame to string type.
    
    Args:
        df (pd.DataFrame): DataFrame to modify.
        columns (list): List of column names to convert to strings.
    
    Returns:
        pd.DataFrame: DataFrame with the specified columns converted to string type.
    """
    for col in columns:
        if col in df.columns:
            df.loc[:, col] = df[col].astype(str).str.strip()
    return df

def handle_duplicate_columns(df, column_base_names):
    """
    Handles duplicate columns generated by merging (e.g., _x and _y suffixes).
    
    Args:
        df (pd.DataFrame): The DataFrame containing potential duplicates.
        column_base_names (list): The base names of the columns to check for duplicates.
    
    Returns:
        pd.DataFrame: DataFrame with duplicates handled.
    """
    for col in column_base_names:
        if f"{col}_x" in df.columns and f"{col}_y" in df.columns:
            # If both _x and _y columns exist, combine them, and drop the original columns
            df[col] = df[f"{col}_x"].combine_first(df[f"{col}_y"])  # Prefer _x, fallback to _y if NaN
            df.drop([f"{col}_x", f"{col}_y"], axis=1, inplace=True)
        elif f"{col}_x" in df.columns:
            df.rename(columns={f"{col}_x": col}, inplace=True)
        elif f"{col}_y" in df.columns:
            df.rename(columns={f"{col}_y": col}, inplace=True)
    return df

async def enrich_data_chunked(main_file, aux_tables_file, output_file, chunk_size=50000):
    """
    Enrich the dataset by joining auxiliary tables in chunks.
    
    Args:
        main_file (str): Path to the main CSV file (e.g., the enriched file with GRUPO_DATAGRO).
        aux_tables_file (str): Path to the Excel file containing auxiliary tables.
        output_file (str): Path to save the enriched CSV file.
        chunk_size (int): Number of rows per chunk to read from the main file.
    """
    try:
        logging.info(f"Starting enrichment of {main_file} using {aux_tables_file}.")
        
        # Load the auxiliary tables
        aux_tables = pd.read_excel(aux_tables_file, sheet_name=None)
        logging.info("Auxiliary tables loaded successfully.")
        
        # Extract relevant tables and drop duplicates
        table_1 = convert_columns_to_str(aux_tables['1'].drop_duplicates(subset=['CO_NCM']), ['CO_NCM'])
        table_10 = convert_columns_to_str(aux_tables['10'].drop_duplicates(subset=['CO_PAIS']), ['CO_PAIS'])
        table_14 = convert_columns_to_str(aux_tables['14'].drop_duplicates(subset=['CO_VIA']), ['CO_VIA'])
        table_6 = convert_columns_to_str(aux_tables['6'].drop_duplicates(subset=['CO_UNID']), ['CO_UNID'])

        chunk_idx = 0
        with pd.read_csv(main_file, delimiter=';', chunksize=chunk_size, dtype=str) as reader:
            for chunk in reader:
                logging.info(f"Processing chunk {chunk_idx + 1}. Columns before merge: {list(chunk.columns)}")

                # Convert relevant columns to strings for consistent merging
                chunk = convert_columns_to_str(chunk, ['CO_NCM', 'CO_PAIS', 'CO_VIA', 'CO_UNID'])

                # Perform the sequential merging with deduplicated tables
                chunk = chunk.merge(table_1[['CO_NCM', 'NO_NCM_POR', 'NO_SH6_POR', 'NO_SH4_POR', 'NO_SH2_POR', 'NO_SEC_POR']], on='CO_NCM', how='left')
                logging.info(f"Columns after merging with table_1: {list(chunk.columns)}")

                chunk = chunk.merge(table_10[['CO_PAIS', 'NO_PAIS']], on='CO_PAIS', how='left')
                logging.info(f"Columns after merging with table_10: {list(chunk.columns)}")

                chunk = chunk.merge(table_14[['CO_VIA', 'NO_VIA']], on='CO_VIA', how='left')
                logging.info(f"Columns after merging with table_14: {list(chunk.columns)}")

                chunk = chunk.merge(table_6[['CO_UNID', 'NO_UNID']], on='CO_UNID', how='left')
                logging.info(f"Columns after merging with table_6: {list(chunk.columns)}")

                # Handle duplicated columns (_x, _y suffixes)
                chunk = handle_duplicate_columns(chunk, ['NO_NCM_POR', 'NO_SH6_POR', 'NO_SH4_POR', 'NO_SH2_POR', 'NO_SEC_POR', 'NO_UNID', 'NO_PAIS', 'NO_VIA'])

                logging.info(f"Columns after handling duplicates: {list(chunk.columns)}")

                # Reorder columns as needed
                if 'VL_FRETE' in chunk.columns:
                    # IMP-specific column order
                    chunk = chunk[['GRUPO_DATAGRO', 'CO_ANO', 'CO_MES', 'CO_NCM', 'NO_NCM_POR', 'NO_SH6_POR', 'NO_SH4_POR', 'NO_SH2_POR', 'NO_SEC_POR', 
                                   'CO_UNID', 'NO_UNID', 'QT_ESTAT', 'KG_LIQUIDO', 'VL_FOB', 'VL_FRETE', 'VL_SEGURO', 
                                   'CO_PAIS', 'NO_PAIS', 'SG_UF_NCM', 'CO_VIA', 'NO_VIA', 'CO_URF']]
                else:
                    # EXP-specific column order
                    chunk = chunk[['GRUPO_DATAGRO', 'CO_ANO', 'CO_MES', 'CO_NCM', 'NO_NCM_POR', 'NO_SH6_POR', 'NO_SH4_POR', 'NO_SH2_POR', 'NO_SEC_POR', 
                                   'CO_UNID', 'NO_UNID', 'QT_ESTAT', 'KG_LIQUIDO', 'VL_FOB', 
                                   'CO_PAIS', 'NO_PAIS', 'SG_UF_NCM', 'CO_VIA', 'NO_VIA', 'CO_URF']]

                # Write the chunk to the output file
                mode = 'w' if chunk_idx == 0 else 'a'
                header = True if chunk_idx == 0 else False
                chunk.to_csv(output_file, mode=mode, index=False, sep=';', header=header)
                
                chunk_idx += 1
        
        logging.info(f"Enrichment complete. Data saved to {output_file}.")
    
    except Exception as e:
        logging.error(f"An error occurred during enrichment: {str(e)}")
        raise

if __name__ == "__main__":
    # Update file paths for both IMP and EXP
    main_file = os.path.join(DATA_DIR, "COMEX-EXP_ENRICHED.csv")  # Adjust this based on dataset type (e.g., COMEX-EXP_ENRICHED.csv)
    aux_tables_file = os.path.join(DATA_DIR, "TABELAS_AUXILIARES.xlsx")
    output_file = os.path.join(DATA_DIR, "COMEX-EXP_FINAL_ENRICHED.csv")  # Adjust this based on dataset type
    
    asyncio.run(enrich_data_chunked(main_file, aux_tables_file, output_file))
